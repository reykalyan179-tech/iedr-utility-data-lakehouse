{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6034f8b-b81e-4788-8f16-3494d809bca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing utility1_circuits.csv → iedr_dev_bronze.utility1_circuits\nSaved 64539 rows to iedr_dev_bronze.utility1_circuits\n+---+-----------------------+-------------------------+-------------------------+---------------------+-------------------+------------------+-------------------+-----------------+-------------------------+------------------+-------------------+-----------------+-----------------+-------------------+-----------------+------------+----------+-------------------------+---------------------+\n|_c0|Circuits_Phase3_CIRCUIT|Circuits_Phase3_NUMPHASES|Circuits_Phase3_OVERUNDER|Circuits_Phase3_PHASE|NYHCPV_csv_NSECTION|NYHCPV_csv_NFEEDER|NYHCPV_csv_NVOLTAGE|NYHCPV_csv_NMAXHC|NYHCPV_csv_NMAPCOLOR     |NYHCPV_csv_FFEEDER|NYHCPV_csv_FVOLTAGE|NYHCPV_csv_FMAXHC|NYHCPV_csv_FMINHC|NYHCPV_csv_FHCADATE|NYHCPV_csv_FNOTES|Shape_Length|utility_id|ingest_timestamp         |source_file          |\n+---+-----------------------+-------------------------+-------------------------+---------------------+-------------------+------------------+-------------------+-----------------+-------------------------+------------------+-------------------+-----------------+-----------------+-------------------+-----------------+------------+----------+-------------------------+---------------------+\n|0  |1105354                |0                        |U                        |                     |200000064          |1105354           |4.8                |0.2              |0.00 TO 0.29 BROWN-953736|1105354           |4.8                |1.8              |0.03             |2022-10-15 00:00:00|NULL             |4.22349E-4  |utility1  |2026-01-19 23:03:00.21789|utility1_circuits.csv|\n|1  |1105352                |3                        |O                        |ABC                  |200000175          |1105352           |4.8                |0.1              |0.00 TO 0.29 BROWN-953736|1105352           |4.8                |3.7              |0.1              |2022-10-15 00:00:00|NULL             |3.65581E-4  |utility1  |2026-01-19 23:03:00.21789|utility1_circuits.csv|\n|2  |1105352                |3                        |O                        |ABC                  |200000176          |1105352           |4.8                |0.1              |0.00 TO 0.29 BROWN-953736|1105352           |4.8                |3.7              |0.1              |2022-10-15 00:00:00|NULL             |3.38696E-4  |utility1  |2026-01-19 23:03:00.21789|utility1_circuits.csv|\n|3  |1105352                |3                        |O                        |ABC                  |200000177          |1105352           |4.8                |0.1              |0.00 TO 0.29 BROWN-953736|1105352           |4.8                |3.7              |0.1              |2022-10-15 00:00:00|NULL             |3.94937E-4  |utility1  |2026-01-19 23:03:00.21789|utility1_circuits.csv|\n|4  |1105352                |3                        |O                        |ABC                  |200000178          |1105352           |4.8                |0.1              |0.00 TO 0.29 BROWN-953736|1105352           |4.8                |3.7              |0.1              |2022-10-15 00:00:00|NULL             |4.96838E-4  |utility1  |2026-01-19 23:03:00.21789|utility1_circuits.csv|\n+---+-----------------------+-------------------------+-------------------------+---------------------+-------------------+------------------+-------------------+-----------------+-------------------------+------------------+-------------------+-----------------+-----------------+-------------------+-----------------+------------+----------+-------------------------+---------------------+\nonly showing top 5 rows\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "\n",
    "file_name     = \"utility1_circuits.csv\"          \n",
    "table_suffix  = \"utility1_circuits\"              \n",
    "utility_label = \"utility1\"                       \n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name  = \"default\"\n",
    "volume_name  = \"iedr_raw\"\n",
    "\n",
    "file_path = f\"/Volumes/workspace/default/iedr_raw/utility1_circuits.csv\"\n",
    "\n",
    "bronze_schema = \"iedr_dev_bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\")\n",
    "\n",
    "print(f\"Processing {file_name} → {bronze_schema}.{table_suffix}\")\n",
    "\n",
    "# Read\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_bronze = df_raw.withColumn(\"utility_id\", lit(utility_label)) \\\n",
    "                   .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "                   .withColumn(\"source_file\", lit(file_name))\n",
    "\n",
    "# Save as Delta table\n",
    "full_table = f\"{bronze_schema}.{table_suffix}\"\n",
    "df_bronze.write.mode(\"append\").format(\"delta\").saveAsTable(full_table)\n",
    "\n",
    "# Verify\n",
    "count = spark.table(full_table).count()\n",
    "print(f\"Saved {count} rows to {full_table}\")\n",
    "spark.table(full_table).show(5, truncate=False)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff98ca4a-d8f5-436a-b7f4-6930adf8f9bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing utility1_install_der.csv → iedr_dev_bronze.utility1_install_der\nSaved 13727 rows to iedr_dev_bronze.utility1_install_der\n+---------+------------+---------------+-----------------+------------------------+---------+-----------------+----------------+------+-------+-------------------+-------+------------+--------------------+------------------+---------+--------+--------------------+----------+-----+------------------------+------------+-----+----------+--------------------------+------------------------+\n|ProjectID|ProjectType |NamePlateRating|TotalChargesCESIR|TotalChargesConstruction|CESIR_EST|SystemUpgrade_EST|ProjectCircuitID|Hybrid|SolarPV|EnergyStorageSystem|Wind   |MicroTurbine|SynchronousGenerator|InductionGenerator|FarmWaste|FuelCell|CombinedHeatandPower|GasTurbine|Hydro|InternalCombustionEngine|SteamTurbine|Other|utility_id|ingest_timestamp          |source_file             |\n+---------+------------+---------------+-----------------+------------------------+---------+-----------------+----------------+------+-------+-------------------+-------+------------+--------------------+------------------+---------+--------+--------------------+----------+-----+------------------------+------------+-----+----------+--------------------------+------------------------+\n|3875     |Transmission|0              |NULL             |NULL                    |NULL     |NULL             |NULL            |N     |0.0    |0.0                |0.0    |0           |0                   |0                 |0.0      |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:06:45.944956|utility1_install_der.csv|\n|7389     |ComWind     |78400          |NULL             |NULL                    |0        |0                |NULL            |N     |0.0    |0.0                |78400.0|0           |0                   |0                 |0.0      |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:06:45.944956|utility1_install_der.csv|\n|11130    |Transmission|NULL           |NULL             |NULL                    |NULL     |NULL             |NULL            |N     |0.0    |0.0                |0.0    |0           |0                   |0                 |0.0      |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:06:45.944956|utility1_install_der.csv|\n|20457    |RESPHOTO    |12             |NULL             |NULL                    |NULL     |NULL             |NULL            |N     |12.0   |0.0                |0.0    |0           |0                   |0                 |0.0      |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:06:45.944956|utility1_install_der.csv|\n|20751    |PVESS       |40.2           |NULL             |NULL                    |NULL     |NULL             |NULL            |Y     |15.2   |25.0               |0.0    |0           |0                   |0                 |0.0      |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:06:45.944956|utility1_install_der.csv|\n+---------+------------+---------------+-----------------+------------------------+---------+-----------------+----------------+------+-------+-------------------+-------+------------+--------------------+------------------+---------+--------+--------------------+----------+-----+------------------------+------------+-----+----------+--------------------------+------------------------+\nonly showing top 5 rows\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "\n",
    "file_name     = \"utility1_install_der.csv\"          \n",
    "table_suffix  = \"utility1_install_der\"              \n",
    "utility_label = \"utility1\"                       \n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name  = \"default\"\n",
    "volume_name  = \"iedr_raw\"\n",
    "\n",
    "file_path = f\"/Volumes/workspace/default/iedr_raw/utility1_install_der.csv\"\n",
    "\n",
    "bronze_schema = \"iedr_dev_bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\")\n",
    "\n",
    "print(f\"Processing {file_name} → {bronze_schema}.{table_suffix}\")\n",
    "\n",
    "# Read\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_bronze = df_raw.withColumn(\"utility_id\", lit(utility_label)) \\\n",
    "                   .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "                   .withColumn(\"source_file\", lit(file_name))\n",
    "\n",
    "# Save as Delta table\n",
    "full_table = f\"{bronze_schema}.{table_suffix}\"\n",
    "df_bronze.write.mode(\"append\").format(\"delta\").saveAsTable(full_table)\n",
    "\n",
    "# Verify\n",
    "count = spark.table(full_table).count()\n",
    "print(f\"Saved {count} rows to {full_table}\")\n",
    "spark.table(full_table).show(5, truncate=False)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e4a8bd-91b0-4a28-b37f-461291db874f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing utility1_planned_der.csv → iedr_dev_bronze.utility1_planned_der\nSaved 1688 rows to iedr_dev_bronze.utility1_planned_der\n+-----------+---------------+-------------+-------------+---------+--------------+----------------+------+-------+-------------------+----+------------+--------------------+------------------+---------+--------+--------------------+----------+-----+------------------------+------------+-----+----------+--------------------------+------------------------+\n|ProjectType|NamePlateRating|InServiceDate|ProjectStatus|ProjectID|CompletionDate|ProjectCircuitID|Hybrid|SolarPV|EnergyStorageSystem|Wind|MicroTurbine|SynchronousGenerator|InductionGenerator|FarmWaste|FuelCell|CombinedHeatandPower|GasTurbine|Hydro|InternalCombustionEngine|SteamTurbine|Other|utility_id|ingest_timestamp          |source_file             |\n+-----------+---------------+-------------+-------------+---------+--------------+----------------+------+-------+-------------------+----+------------+--------------------+------------------+---------+--------+--------------------+----------+-----+------------------------+------------+-----+----------+--------------------------+------------------------+\n|NRESPHOTO  |2000           |NULL         |NULL         |10006    |NULL          |NULL            |N     |2000.0 |0.0                |0.0 |0           |0                   |0                 |0        |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:07:47.207924|utility1_planned_der.csv|\n|NRESPHOTO  |5000           |NULL         |NULL         |10011    |NULL          |NULL            |N     |5000.0 |0.0                |0.0 |0           |0                   |0                 |0        |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:07:47.207924|utility1_planned_der.csv|\n|NRESPHOTO  |68.4           |NULL         |NULL         |10125    |NULL          |NULL            |N     |68.4   |0.0                |0.0 |0           |0                   |0                 |0        |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:07:47.207924|utility1_planned_der.csv|\n|NRESPHOTO  |2000           |NULL         |NULL         |10128    |NULL          |NULL            |N     |2000.0 |0.0                |0.0 |0           |0                   |0                 |0        |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:07:47.207924|utility1_planned_der.csv|\n|NRESPHOTO  |5000           |NULL         |NULL         |10144    |NULL          |NULL            |N     |5000.0 |0.0                |0.0 |0           |0                   |0                 |0        |0       |0.0                 |0         |0    |0                       |0           |0    |utility1  |2026-01-19 23:07:47.207924|utility1_planned_der.csv|\n+-----------+---------------+-------------+-------------+---------+--------------+----------------+------+-------+-------------------+----+------------+--------------------+------------------+---------+--------+--------------------+----------+-----+------------------------+------------+-----+----------+--------------------------+------------------------+\nonly showing top 5 rows\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "\n",
    "file_name     = \"utility1_planned_der.csv\"          \n",
    "table_suffix  = \"utility1_planned_der\"              \n",
    "utility_label = \"utility1\"                       \n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name  = \"default\"\n",
    "volume_name  = \"iedr_raw\"\n",
    "\n",
    "file_path = f\"/Volumes/workspace/default/iedr_raw/utility1_planned_der.csv\"\n",
    "\n",
    "bronze_schema = \"iedr_dev_bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\")\n",
    "\n",
    "print(f\"Processing {file_name} → {bronze_schema}.{table_suffix}\")\n",
    "\n",
    "# Read\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_bronze = df_raw.withColumn(\"utility_id\", lit(utility_label)) \\\n",
    "                   .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "                   .withColumn(\"source_file\", lit(file_name))\n",
    "\n",
    "# Save as Delta table\n",
    "full_table = f\"{bronze_schema}.{table_suffix}\"\n",
    "df_bronze.write.mode(\"append\").format(\"delta\").saveAsTable(full_table)\n",
    "\n",
    "# Verify\n",
    "count = spark.table(full_table).count()\n",
    "print(f\"Saved {count} rows to {full_table}\")\n",
    "spark.table(full_table).show(5, truncate=False)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05f2323-ada4-418a-b324-f15fdb1c6c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing utility2_circuits.csv → iedr_dev_bronze.utility2_circuits\nSaved 1909 rows to iedr_dev_bronze.utility2_circuits\n+-----------+--------------+-------------+-------------+---------------------------------+----------------------+---------+------------+----------+--------------------------+---------------------+\n|Master_CDF |feeder_voltage|feeder_max_hc|feeder_min_hc|feeder_dg_connected_since_refresh|hca_refresh_date      |color    |shape_length|utility_id|ingest_timestamp          |source_file          |\n+-----------+--------------+-------------+-------------+---------------------------------+----------------------+---------+------------+----------+--------------------------+---------------------+\n|36_13_81756|13.2          |0.0          |0.0          |0.0                              |2022/10/01 00:00:00+00|brown    |1.277295106 |utility2  |2026-01-19 23:04:26.788543|utility2_circuits.csv|\n|36_13_81757|13.2          |1.1          |0.1          |0.01                             |2022/10/01 00:00:00+00|green    |3.175783235 |utility2  |2026-01-19 23:04:26.788543|utility2_circuits.csv|\n|36_13_81758|13.2          |8.72         |1.2          |0.0                              |2022/10/01 00:00:00+00|dark blue|0.238415688 |utility2  |2026-01-19 23:04:26.788543|utility2_circuits.csv|\n|36_13_87551|13.2          |0.0          |0.0          |0.0                              |2022/10/01 00:00:00+00|brown    |2.064842384 |utility2  |2026-01-19 23:04:26.788543|utility2_circuits.csv|\n|36_13_87552|13.2          |1.33         |0.04         |0.0                              |2022/10/01 00:00:00+00|green    |1.529848608 |utility2  |2026-01-19 23:04:26.788543|utility2_circuits.csv|\n+-----------+--------------+-------------+-------------+---------------------------------+----------------------+---------+------------+----------+--------------------------+---------------------+\nonly showing top 5 rows\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "\n",
    "file_name     = \"utility2_circuits.csv\"          \n",
    "table_suffix  = \"utility2_circuits\"              \n",
    "utility_label = \"utility2\"                       \n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name  = \"default\"\n",
    "volume_name  = \"iedr_raw\"\n",
    "\n",
    "file_path = f\"/Volumes/workspace/default/iedr_raw/utility2_circuits.csv\"\n",
    "\n",
    "bronze_schema = \"iedr_dev_bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\")\n",
    "\n",
    "print(f\"Processing {file_name} → {bronze_schema}.{table_suffix}\")\n",
    "\n",
    "# Read\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_bronze = df_raw.withColumn(\"utility_id\", lit(utility_label)) \\\n",
    "                   .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "                   .withColumn(\"source_file\", lit(file_name))\n",
    "\n",
    "# Save as Delta table\n",
    "full_table = f\"{bronze_schema}.{table_suffix}\"\n",
    "df_bronze.write.mode(\"append\").format(\"delta\").saveAsTable(full_table)\n",
    "\n",
    "# Verify\n",
    "count = spark.table(full_table).count()\n",
    "print(f\"Saved {count} rows to {full_table}\")\n",
    "spark.table(full_table).show(5, truncate=False)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386573c9-b665-48ce-bd16-39b016bac27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing utility2_install_der.csv → iedr_dev_bronze.utility2_install_der\nSaved 25537 rows to iedr_dev_bronze.utility2_install_der\n+------+----------------------+--------+--------------------+----------------------------+--------------------+----------+--------------------------+------------------------+\n|DER_ID|SERVICE_STREET_ADDRESS|DER_TYPE|DER_NAMEPLATE_RATING|DER_INTERCONNECTION_LOCATION|INTERCONNECTION_COST|utility_id|ingest_timestamp          |source_file             |\n+------+----------------------+--------+--------------------+----------------------------+--------------------+----------+--------------------------+------------------------+\n|391308|null                  |Solar   |7.6                 |36_39_01251                 |0.0                 |utility2  |2026-01-19 23:08:23.670109|utility2_install_der.csv|\n|386550|null                  |Solar   |6.0                 |36_32_36452                 |0.0                 |utility2  |2026-01-19 23:08:23.670109|utility2_install_der.csv|\n|390019|null                  |Solar   |7.0                 |36_32_36051                 |0.0                 |utility2  |2026-01-19 23:08:23.670109|utility2_install_der.csv|\n|398157|null                  |Solar   |9.0                 |36_01_20655                 |0.0                 |utility2  |2026-01-19 23:08:23.670109|utility2_install_der.csv|\n|388371|null                  |Solar   |7.6                 |36_10_2562                  |0.0                 |utility2  |2026-01-19 23:08:23.670109|utility2_install_der.csv|\n+------+----------------------+--------+--------------------+----------------------------+--------------------+----------+--------------------------+------------------------+\nonly showing top 5 rows\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "\n",
    "file_name     = \"utility2_install_der.csv\"          \n",
    "table_suffix  = \"utility2_install_der\"              \n",
    "utility_label = \"utility2\"                       \n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name  = \"default\"\n",
    "volume_name  = \"iedr_raw\"\n",
    "\n",
    "file_path = f\"/Volumes/workspace/default/iedr_raw/utility2_install_der.csv\"\n",
    "\n",
    "bronze_schema = \"iedr_dev_bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\")\n",
    "\n",
    "print(f\"Processing {file_name} → {bronze_schema}.{table_suffix}\")\n",
    "\n",
    "# Read\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_bronze = df_raw.withColumn(\"utility_id\", lit(utility_label)) \\\n",
    "                   .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "                   .withColumn(\"source_file\", lit(file_name))\n",
    "\n",
    "# Save as Delta table\n",
    "full_table = f\"{bronze_schema}.{table_suffix}\"\n",
    "df_bronze.write.mode(\"append\").format(\"delta\").saveAsTable(full_table)\n",
    "\n",
    "# Verify\n",
    "count = spark.table(full_table).count()\n",
    "print(f\"Saved {count} rows to {full_table}\")\n",
    "spark.table(full_table).show(5, truncate=False)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "319bd99d-867f-412e-9819-f2949e0fb6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing utility2_planned_der.csv → iedr_dev_bronze.utility2_planned_der\nSaved 30957 rows to iedr_dev_bronze.utility2_planned_der\n+--------+--------------------+-------------------------+-------------------------+----------+--------------------+-----------------------+--------------------------------+------------------------------+----------------------------+----------+--------------------------+------------------------+\n|DER_TYPE|DER_NAMEPLATE_RATING|INVERTER_NAMEPLATE_RATING|PLANNED_INSTALLATION_DATE|DER_STATUS|DER_STATUS_RATIONALE|TOTAL_MW_FOR_SUBSTATION|INTERCONNECTION_QUEUE_REQUEST_ID|INTERCONNECTION_QUEUE_POSITION|DER_INTERCONNECTION_LOCATION|utility_id|ingest_timestamp          |source_file             |\n+--------+--------------------+-------------------------+-------------------------+----------+--------------------+-----------------------+--------------------------------+------------------------------+----------------------------+----------+--------------------------+------------------------+\n|Solar   |5000.0              |null                     |2020-11-27               |null      |null                |NULL                   |293870                          |6/5/2020 13:47                |null                        |utility2  |2026-01-19 23:09:09.312023|utility2_planned_der.csv|\n|Solar   |11.4                |null                     |2020-07-24               |null      |null                |11.4                   |300474                          |7/13/2020 11:54               |36_32_36552                 |utility2  |2026-01-19 23:09:09.312023|utility2_planned_der.csv|\n|Solar   |5000.0              |null                     |2022-10-28               |null      |null                |NULL                   |300707                          |7/23/2020 19:49               |null                        |utility2  |2026-01-19 23:09:09.312023|utility2_planned_der.csv|\n|Solar   |7.6                 |null                     |2020-07-24               |null      |null                |7.6                    |300492                          |7/13/2020 12:49               |36_35_39252                 |utility2  |2026-01-19 23:09:09.312023|utility2_planned_der.csv|\n|Solar   |5000.0              |null                     |2021-12-31               |null      |null                |NULL                   |300720                          |7/20/2020 18:56               |null                        |utility2  |2026-01-19 23:09:09.312023|utility2_planned_der.csv|\n+--------+--------------------+-------------------------+-------------------------+----------+--------------------+-----------------------+--------------------------------+------------------------------+----------------------------+----------+--------------------------+------------------------+\nonly showing top 5 rows\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "\n",
    "file_name     = \"utility2_planned_der.csv\"          \n",
    "table_suffix  = \"utility2_planned_der\"              \n",
    "utility_label = \"utility2\"                       \n",
    "\n",
    "catalog_name = \"workspace\"\n",
    "schema_name  = \"default\"\n",
    "volume_name  = \"iedr_raw\"\n",
    "\n",
    "file_path = f\"/Volumes/workspace/default/iedr_raw/utility2_planned_der.csv\"\n",
    "\n",
    "bronze_schema = \"iedr_dev_bronze\"\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {bronze_schema}\")\n",
    "\n",
    "print(f\"Processing {file_name} → {bronze_schema}.{table_suffix}\")\n",
    "\n",
    "# Read\n",
    "df_raw = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "\n",
    "# Add metadata columns\n",
    "df_bronze = df_raw.withColumn(\"utility_id\", lit(utility_label)) \\\n",
    "                   .withColumn(\"ingest_timestamp\", current_timestamp()) \\\n",
    "                   .withColumn(\"source_file\", lit(file_name))\n",
    "\n",
    "# Save as Delta table \n",
    "full_table = f\"{bronze_schema}.{table_suffix}\"\n",
    "df_bronze.write.mode(\"append\").format(\"delta\").saveAsTable(full_table)\n",
    "\n",
    "# Verify\n",
    "count = spark.table(full_table).count()\n",
    "print(f\"Saved {count} rows to {full_table}\")\n",
    "spark.table(full_table).show(5, truncate=False)\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe508e7b-f601-4000-9b57-8cd6988f8bc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Layer Summary\n------------------------------------------------------------\nTable: iedr_dev_bronze.utility1_circuits\n  Rows: 64,539\n  Nulls in sample columns + metadata:\n\nExample Schema (utility1_circuits):\nroot\n |-- _c0: integer (nullable = true)\n |-- Circuits_Phase3_CIRCUIT: integer (nullable = true)\n |-- Circuits_Phase3_NUMPHASES: integer (nullable = true)\n |-- Circuits_Phase3_OVERUNDER: string (nullable = true)\n |-- Circuits_Phase3_PHASE: string (nullable = true)\n |-- NYHCPV_csv_NSECTION: integer (nullable = true)\n |-- NYHCPV_csv_NFEEDER: integer (nullable = true)\n |-- NYHCPV_csv_NVOLTAGE: double (nullable = true)\n |-- NYHCPV_csv_NMAXHC: double (nullable = true)\n |-- NYHCPV_csv_NMAPCOLOR: string (nullable = true)\n |-- NYHCPV_csv_FFEEDER: integer (nullable = true)\n |-- NYHCPV_csv_FVOLTAGE: double (nullable = true)\n |-- NYHCPV_csv_FMAXHC: double (nullable = true)\n |-- NYHCPV_csv_FMINHC: double (nullable = true)\n |-- NYHCPV_csv_FHCADATE: timestamp (nullable = true)\n |-- NYHCPV_csv_FNOTES: string (nullable = true)\n |-- Shape_Length: double (nullable = true)\n |-- utility_id: string (nullable = true)\n |-- ingest_timestamp: timestamp (nullable = true)\n |-- source_file: string (nullable = true)\n\n------------------------------------------------------------\nTable: iedr_dev_bronze.utility1_install_der\n  Rows: 13,727\n  Nulls in sample columns + metadata:\n------------------------------------------------------------\nTable: iedr_dev_bronze.utility1_planned_der\n  Rows: 1,688\n  Nulls in sample columns + metadata:\n------------------------------------------------------------\nTable: iedr_dev_bronze.utility2_circuits\n  Rows: 1,909\n  Nulls in sample columns + metadata:\n------------------------------------------------------------\nTable: iedr_dev_bronze.utility2_install_der\n  Rows: 25,537\n  Nulls in sample columns + metadata:\n    DER_NAMEPLATE_RATING: 27\n------------------------------------------------------------\nTable: iedr_dev_bronze.utility2_planned_der\n  Rows: 30,957\n  Nulls in sample columns + metadata:\n    DER_NAMEPLATE_RATING: 33\n    PLANNED_INSTALLATION_DATE: 20,161\n------------------------------------------------------------\nBronze quality check complete!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, sum as spark_sum\n",
    "\n",
    "bronze_schema = \"iedr_dev_bronze\"\n",
    "\n",
    "tables = [\n",
    "    \"utility1_circuits\",\n",
    "    \"utility1_install_der\",\n",
    "    \"utility1_planned_der\",\n",
    "    \"utility2_circuits\",\n",
    "    \"utility2_install_der\",\n",
    "    \"utility2_planned_der\"\n",
    "]\n",
    "\n",
    "print(\"Bronze Layer Summary\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for tbl in tables:\n",
    "    full_tbl = f\"{bronze_schema}.{tbl}\"\n",
    "    try:\n",
    "        df = spark.table(full_tbl)\n",
    "        row_count = df.count()\n",
    "        print(f\"Table: {full_tbl}\")\n",
    "        print(f\"  Rows: {row_count:,}\")\n",
    "        \n",
    "        # Quick null check on first 5 columns + metadata\n",
    "        null_counts = df.select([\n",
    "            spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "            for c in df.columns[:5] + [\"utility_id\", \"ingest_timestamp\"]\n",
    "        ]).collect()[0]\n",
    "        \n",
    "        print(\"  Nulls in sample columns + metadata:\")\n",
    "        for c, cnt in zip(null_counts.asDict().keys(), null_counts):\n",
    "            if cnt > 0:\n",
    "                print(f\"    {c}: {cnt:,}\")\n",
    "        \n",
    "        # Show schema once for first table\n",
    "        if tbl == tables[0]:\n",
    "            print(\"\\nExample Schema (utility1_circuits):\")\n",
    "            df.printSchema()\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking {full_tbl}: {str(e)}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "print(\"Bronze quality check complete!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}