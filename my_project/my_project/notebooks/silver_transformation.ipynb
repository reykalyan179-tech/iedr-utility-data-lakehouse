{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a54b7a7-f095-4aab-9f61-50858fa99580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in environment: prod\nReading from bronze schema: iedr_prod_bronze\nWriting to silver schema: iedr_prod_silver\nPlatinum schema : iedr_prod_platinum\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dbutils.widgets.dropdown(\"env\", \"dev\", [\"dev\", \"qa\", \"prod\"])\n",
    "env = dbutils.widgets.get(\"env\")\n",
    "\n",
    "bronze_schema = f\"iedr_{env}_bronze\"   \n",
    "silver_schema = f\"iedr_{env}_silver\"   \n",
    "platinum_schema = f\"iedr_{env}_platinum\"\n",
    "\n",
    "print(f\"Running in environment: {env}\")\n",
    "print(f\"Reading from bronze schema: {bronze_schema}\")\n",
    "print(f\"Writing to silver schema: {silver_schema}\")\n",
    "print(f\"Platinum schema : {platinum_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2947855a-b155-479f-b589-3eb31b834f4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, max as spark_max, count, lit, when, coalesce, to_timestamp\n",
    "\n",
    "# Bronze schema\n",
    "#bronze_schema = \"iedr_dev_bronze\"\n",
    "\n",
    "# Create silver schema\n",
    "#silver_schema = \"iedr_dev_silver\"\n",
    "#spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {silver_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b455572-0761-44bd-9970-5ddd6f1f7d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 2,176\n+---------+-----------------------+-------------+----------+------------+-------------------+\n|feeder_id|max_hosting_capacity_mw|segment_count|utility_id|source_grain|refresh_date       |\n+---------+-----------------------+-------------+----------+------------+-------------------+\n|1105004  |10.0                   |446          |utility1  |segment     |2022-10-15 00:00:00|\n|2300303  |10.0                   |291          |utility1  |segment     |2022-10-15 00:00:00|\n|1501802  |1.3                    |262          |utility1  |segment     |2022-10-15 00:00:00|\n|1501601  |0.4                    |660          |utility1  |segment     |2022-10-15 00:00:00|\n|1501001  |0.3                    |58           |utility1  |segment     |2022-06-30 00:00:00|\n+---------+-----------------------+-------------+----------+------------+-------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# --- Silver Feeders Table ---\n",
    "\n",
    "u1_circuits = spark.table(f\"{bronze_schema}.utility1_circuits\")\n",
    "\n",
    "silver_feeders_u1 = u1_circuits.groupBy(\"NYHCPV_csv_NFEEDER\") \\\n",
    "    .agg(\n",
    "        spark_max(\"NYHCPV_csv_NMAXHC\").alias(\"max_hosting_capacity_mw\"),\n",
    "        count(\"*\").alias(\"segment_count\"),\n",
    "        spark_max(\"NYHCPV_csv_FHCADATE\").alias(\"refresh_date\")  # latest date as proxy\n",
    "    ) \\\n",
    "    .withColumn(\"feeder_id\", col(\"NYHCPV_csv_NFEEDER\").cast(\"string\")) \\\n",
    "    .withColumn(\"utility_id\", lit(\"utility1\")) \\\n",
    "    .withColumn(\"source_grain\", lit(\"segment\")) \\\n",
    "    .select(\"feeder_id\", \"max_hosting_capacity_mw\", \"segment_count\", \"utility_id\", \"source_grain\", \"refresh_date\")\n",
    "\n",
    "u2_circuits = spark.table(f\"{bronze_schema}.utility2_circuits\")\n",
    "\n",
    "silver_feeders_u2 = u2_circuits.select(\n",
    "    col(\"Master_CDF\").alias(\"feeder_id\"),\n",
    "    col(\"feeder_max_hc\").alias(\"max_hosting_capacity_mw\"),\n",
    "    lit(1).alias(\"segment_count\"),\n",
    "    lit(\"utility2\").alias(\"utility_id\"),\n",
    "    lit(\"feeder\").alias(\"source_grain\"),\n",
    "    to_timestamp(\n",
    "        col(\"hca_refresh_date\"),\n",
    "        \"yyyy/MM/dd HH:mm:ssX\"  \n",
    "    ).alias(\"refresh_date\")\n",
    ")\n",
    "\n",
    "silver_feeders = silver_feeders_u1.unionByName(silver_feeders_u2)\n",
    "\n",
    "silver_feeders.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.feeders\")\n",
    "\n",
    "print(f\"Rows: {silver_feeders.count():,}\")\n",
    "silver_feeders.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3535f0b3-e1ef-4ad5-83ba-7b3d334c775d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver DER records table created\nTotal DER rows: 71,909\n\nBreakdown by utility and status:\n+----------+---------+-----+\n|utility_id|   status|count|\n+----------+---------+-----+\n|  utility1|installed|13727|\n|  utility1|  planned| 1688|\n|  utility2|installed|25537|\n|  utility2|  planned|30957|\n+----------+---------+-----+\n\n\nSample 5 rows:\n+---------+--------+--------------------+-------------------+---------+----------+----------+--------------------------+\n|feeder_id|der_type|nameplate_rating_raw|nameplate_rating_mw|status   |project_id|utility_id|ingest_timestamp          |\n+---------+--------+--------------------+-------------------+---------+----------+----------+--------------------------+\n|NULL     |Other   |0.0                 |0.0                |installed|3875      |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Wind    |78400.0             |78.4               |installed|7389      |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Other   |NULL                |NULL               |installed|11130     |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Solar   |12.0                |0.012              |installed|20457     |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Other   |40.2                |0.0402             |installed|20751     |utility1  |2026-01-20 03:34:32.112254|\n+---------+--------+--------------------+-------------------+---------+----------+----------+--------------------------+\nonly showing top 5 rows\n\nSilver DER transformation complete!\n"
     ]
    }
   ],
   "source": [
    "# --- Silver DER Records Table (Installed + Planned Unified) ---\n",
    "\n",
    "from pyspark.sql.functions import col, lit, when, substring_index, coalesce, expr\n",
    "\n",
    "der_schema = silver_schema\n",
    "\n",
    "# Utility1 Installed DER\n",
    "u1_installed = spark.table(f\"{bronze_schema}.utility1_install_der\")\n",
    "\n",
    "silver_u1_installed = u1_installed.select(\n",
    "    coalesce(\n",
    "        col(\"ProjectCircuitID\"),\n",
    "        substring_index(col(\"ProjectCircuitID\"), \",\", 1)\n",
    "    ).alias(\"feeder_id\"),\n",
    "    \n",
    "    when(col(\"ProjectType\").isin(\"RESPHOTO\", \"NRESPHOTO\"), \"Solar\")\n",
    "     .when(col(\"ProjectType\") == \"ESS\", \"EnergyStorage\")\n",
    "     .when(col(\"ProjectType\") == \"ComSolar\", \"Solar\")\n",
    "     .when(col(\"ProjectType\") == \"ComWind\", \"Wind\")\n",
    "     .when(col(\"ProjectType\") == \"HYDRO\", \"Hydro\")\n",
    "     .otherwise(\"Other\").alias(\"der_type\"),\n",
    "    \n",
    "    # Safe cast using SQL try_cast\n",
    "    expr(\"try_cast(NamePlateRating AS DOUBLE)\").alias(\"nameplate_rating_raw\"),\n",
    "    expr(\"try_cast(NamePlateRating AS DOUBLE) / 1000\").alias(\"nameplate_rating_mw\"),\n",
    "    \n",
    "    lit(\"installed\").alias(\"status\"),\n",
    "    col(\"ProjectID\").alias(\"project_id\"),\n",
    "    lit(\"utility1\").alias(\"utility_id\"),\n",
    "    col(\"ingest_timestamp\")\n",
    ").filter(\n",
    "    col(\"feeder_id\").isNotNull()\n",
    ")\n",
    "\n",
    "# Utility1 Planned DER\n",
    "u1_planned = spark.table(f\"{bronze_schema}.utility1_planned_der\")\n",
    "\n",
    "silver_u1_planned = u1_planned.select(\n",
    "    col(\"ProjectCircuitID\").alias(\"feeder_id\"),\n",
    "    \n",
    "    when(col(\"ProjectType\") == \"NRESPHOTO\", \"Solar\")\n",
    "     .when(col(\"ProjectType\") == \"ComSolar\", \"Solar\")\n",
    "     .when(col(\"ProjectType\") == \"ESS\", \"EnergyStorage\")\n",
    "     .when(col(\"ProjectType\") == \"ComWind\", \"Wind\")\n",
    "     .when(col(\"ProjectType\") == \"HYDRO\", \"Hydro\")\n",
    "     .otherwise(\"Other\").alias(\"der_type\"),\n",
    "    \n",
    "    expr(\"try_cast(NamePlateRating AS DOUBLE)\").alias(\"nameplate_rating_raw\"),\n",
    "    expr(\"try_cast(NamePlateRating AS DOUBLE) / 1000\").alias(\"nameplate_rating_mw\"),\n",
    "    \n",
    "    lit(\"planned\").alias(\"status\"),\n",
    "    col(\"ProjectID\").alias(\"project_id\"),\n",
    "    lit(\"utility1\").alias(\"utility_id\"),\n",
    "    col(\"ingest_timestamp\")\n",
    ").filter(\n",
    "    col(\"feeder_id\").isNotNull()\n",
    ")\n",
    "\n",
    "# Utility2 Installed DER\n",
    "u2_installed = spark.table(f\"{bronze_schema}.utility2_install_der\")\n",
    "\n",
    "silver_u2_installed = u2_installed.select(\n",
    "    col(\"DER_INTERCONNECTION_LOCATION\").alias(\"feeder_id\"),\n",
    "    col(\"DER_TYPE\").alias(\"der_type\"),\n",
    "    \n",
    "    expr(\"try_cast(DER_NAMEPLATE_RATING AS DOUBLE)\").alias(\"nameplate_rating_mw\"),\n",
    "    \n",
    "    lit(\"installed\").alias(\"status\"),\n",
    "    col(\"DER_ID\").alias(\"project_id\"),\n",
    "    lit(\"utility2\").alias(\"utility_id\"),\n",
    "    col(\"ingest_timestamp\")\n",
    ").filter(col(\"feeder_id\").isNotNull())\n",
    "\n",
    "# Utility2 Planned DER\n",
    "u2_planned = spark.table(f\"{bronze_schema}.utility2_planned_der\")\n",
    "\n",
    "silver_u2_planned = u2_planned.select(\n",
    "    col(\"DER_INTERCONNECTION_LOCATION\").alias(\"feeder_id\"),\n",
    "    col(\"DER_TYPE\").alias(\"der_type\"),\n",
    "    \n",
    "    expr(\"try_cast(DER_NAMEPLATE_RATING AS DOUBLE)\").alias(\"nameplate_rating_mw\"),\n",
    "    \n",
    "    lit(\"planned\").alias(\"status\"),\n",
    "    col(\"INTERCONNECTION_QUEUE_REQUEST_ID\").alias(\"project_id\"),\n",
    "    lit(\"utility2\").alias(\"utility_id\"),\n",
    "    col(\"ingest_timestamp\")\n",
    ").filter(col(\"feeder_id\").isNotNull())\n",
    "\n",
    "# Union all\n",
    "silver_der_records = (\n",
    "    silver_u1_installed\n",
    "    .unionByName(silver_u1_planned, allowMissingColumns=True)\n",
    "    .unionByName(silver_u2_installed, allowMissingColumns=True)\n",
    "    .unionByName(silver_u2_planned, allowMissingColumns=True)\n",
    ")\n",
    "\n",
    "# Save\n",
    "silver_der_records.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{silver_schema}.der_records\")\n",
    "\n",
    "# Summary\n",
    "print(\"Silver DER records table created\")\n",
    "total_rows = silver_der_records.count()\n",
    "print(f\"Total DER rows: {total_rows:,}\")\n",
    "\n",
    "print(\"\\nBreakdown by utility and status:\")\n",
    "silver_der_records.groupBy(\"utility_id\", \"status\").count().show()\n",
    "\n",
    "print(\"\\nSample 5 rows:\")\n",
    "silver_der_records.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nSilver DER transformation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e33a1ff-d6a7-4aae-9771-0c5b10c3b9b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platinum feeder_capacity created\nRows: 2,176\n+-----------+-----------------------+-------------+----------+------------+-------------------+-------------------+-----------------+----------------+\n|feeder_id  |max_hosting_capacity_mw|segment_count|utility_id|source_grain|refresh_date       |installed_der_count|planned_der_count|unique_der_types|\n+-----------+-----------------------+-------------+----------+------------+-------------------+-------------------+-----------------+----------------+\n|36_14_25453|10.0                   |1            |utility2  |feeder      |2022-10-01 00:00:00|9                  |10               |1               |\n|36_17_64656|10.0                   |1            |utility2  |feeder      |2022-10-01 00:00:00|18                 |29               |1               |\n|36_14_29951|10.0                   |1            |utility2  |feeder      |2022-10-01 00:00:00|4                  |6                |1               |\n|36_14_25458|10.0                   |1            |utility2  |feeder      |2022-10-01 00:00:00|18                 |25               |1               |\n|36_17_64655|10.0                   |1            |utility2  |feeder      |2022-10-01 00:00:00|16                 |23               |1               |\n+-----------+-----------------------+-------------+----------+------------+-------------------+-------------------+-----------------+----------------+\nonly showing top 5 rows\n\nPlatinum der_details created\nRows: 71,909\n+---------+--------+-------------------+---------+----------+----------+--------------------------+\n|feeder_id|der_type|nameplate_rating_mw|status   |project_id|utility_id|ingest_timestamp          |\n+---------+--------+-------------------+---------+----------+----------+--------------------------+\n|NULL     |Other   |0.0                |installed|3875      |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Wind    |78.4               |installed|7389      |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Other   |NULL               |installed|11130     |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Solar   |0.012              |installed|20457     |utility1  |2026-01-20 03:34:32.112254|\n|NULL     |Other   |0.0402             |installed|20751     |utility1  |2026-01-20 03:34:32.112254|\n+---------+--------+-------------------+---------+----------+----------+--------------------------+\nonly showing top 5 rows\n\nData Quality Summary:\n+--------------------------+---------+---------------+----------------------+\n|last_refresh              |row_count|table          |note                  |\n+--------------------------+---------+---------------+----------------------+\n|2022-10-15 00:00:00       |2176     |feeders        |NULL                  |\n|2026-01-20 03:34:50.066774|71909    |der_records    |NULL                  |\n|NULL                      |2176     |feeder_capacity|Aggregated from silver|\n|NULL                      |71909    |der_details    |Detailed DER view     |\n+--------------------------+---------+---------------+----------------------+\n\n\nPlatinum layer complete!\n"
     ]
    }
   ],
   "source": [
    "# Platinum Layer (API-ready tables)\n",
    "\n",
    "from pyspark.sql.functions import sum as spark_sum, countDistinct\n",
    "\n",
    "#platinum_schema = \"iedr_dev_platinum\"\n",
    "#spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {platinum_schema}\")\n",
    "\n",
    "# 1. Feeder Capacity Table (with DER counts)\n",
    "silver_feeders = spark.table(f\"{silver_schema}.feeders\")\n",
    "silver_ders = spark.table(f\"{silver_schema}.der_records\")\n",
    "\n",
    "# Aggregate DER counts per feeder\n",
    "der_counts = silver_ders.groupBy(\"feeder_id\").agg(\n",
    "    spark_sum(when(col(\"status\") == \"installed\", 1).otherwise(0)).alias(\"installed_der_count\"),\n",
    "    spark_sum(when(col(\"status\") == \"planned\", 1).otherwise(0)).alias(\"planned_der_count\"),\n",
    "    countDistinct(\"der_type\").alias(\"unique_der_types\")\n",
    ")\n",
    "\n",
    "platinum_feeders = silver_feeders.join(\n",
    "    der_counts,\n",
    "    \"feeder_id\",\n",
    "    \"left_outer\"\n",
    ").fillna({\"installed_der_count\": 0, \"planned_der_count\": 0, \"unique_der_types\": 0})\n",
    "\n",
    "platinum_feeders.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{platinum_schema}.feeder_capacity\")\n",
    "\n",
    "print(\"Platinum feeder_capacity created\")\n",
    "print(f\"Rows: {platinum_feeders.count():,}\")\n",
    "platinum_feeders.orderBy(col(\"max_hosting_capacity_mw\").desc()).show(5, truncate=False)\n",
    "\n",
    "# 2. DER Details Table (simple copy with some cleanup)\n",
    "platinum_der_details = silver_ders.select(\n",
    "    \"feeder_id\",\n",
    "    \"der_type\",\n",
    "    \"nameplate_rating_mw\",\n",
    "    \"status\",\n",
    "    \"project_id\",\n",
    "    \"utility_id\",\n",
    "    \"ingest_timestamp\"\n",
    ")\n",
    "\n",
    "platinum_der_details.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{platinum_schema}.der_details\")\n",
    "\n",
    "print(\"\\nPlatinum der_details created\")\n",
    "print(f\"Rows: {platinum_der_details.count():,}\")\n",
    "platinum_der_details.show(5, truncate=False)\n",
    "\n",
    "# 3. Data Quality Summary (as per task: refresh dates, volume, missing data)\n",
    "quality_summary = spark.createDataFrame([\n",
    "    {\"table\": \"feeders\", \"row_count\": silver_feeders.count(), \"last_refresh\": silver_feeders.agg(spark_max(\"refresh_date\")).collect()[0][0]},\n",
    "    {\"table\": \"der_records\", \"row_count\": silver_ders.count(), \"last_refresh\": silver_ders.agg(spark_max(\"ingest_timestamp\")).collect()[0][0]},\n",
    "    {\"table\": \"feeder_capacity\", \"row_count\": platinum_feeders.count(), \"note\": \"Aggregated from silver\"},\n",
    "    {\"table\": \"der_details\", \"row_count\": platinum_der_details.count(), \"note\": \"Detailed DER view\"}\n",
    "])\n",
    "\n",
    "quality_summary.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{platinum_schema}.data_quality_summary\")\n",
    "\n",
    "print(\"\\nData Quality Summary:\")\n",
    "quality_summary.show(truncate=False)\n",
    "\n",
    "print(\"\\nPlatinum layer complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda86ae1-949d-4ebc-a11b-14f8e3ca9ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Feeders with max_hosting_capacity_mw > 5 (platinum.feeder_capacity)\n+---------+-----------------------+-------------+----------+------------+-------------------+-------------------+-----------------+----------------+\n|feeder_id|max_hosting_capacity_mw|segment_count|utility_id|source_grain|refresh_date       |installed_der_count|planned_der_count|unique_der_types|\n+---------+-----------------------+-------------+----------+------------+-------------------+-------------------+-----------------+----------------+\n|2307210  |10.0                   |2400         |utility1  |segment     |2022-10-15 00:00:00|84                 |4                |2               |\n|1107241  |10.0                   |1785         |utility1  |segment     |2022-10-15 00:00:00|108                |12               |2               |\n|1109706  |10.0                   |1415         |utility1  |segment     |2022-10-15 00:00:00|140                |12               |2               |\n|1105249  |10.0                   |800          |utility1  |segment     |2022-10-15 00:00:00|32                 |4                |2               |\n|2300303  |10.0                   |1455         |utility1  |segment     |2022-10-15 00:00:00|28                 |4                |1               |\n|1107731  |10.0                   |1085         |utility1  |segment     |2022-10-15 00:00:00|144                |16               |3               |\n|1105004  |10.0                   |2230         |utility1  |segment     |2022-10-15 00:00:00|56                 |8                |2               |\n|2307010  |10.0                   |1995         |utility1  |segment     |2022-10-15 00:00:00|60                 |4                |2               |\n|1105251  |10.0                   |550          |utility1  |segment     |2022-10-15 00:00:00|72                 |16               |2               |\n|1505002  |10.0                   |1090         |utility1  |segment     |2022-10-15 00:00:00|20                 |12               |1               |\n+---------+-----------------------+-------------+----------+------------+-------------------+-------------------+-----------------+----------------+\n\n\nTest 2: All installed + planned DER for a specific feeder (platinum.der_details) \n+-----------+--------+-------------------+---------+----------+----------+\n|feeder_id  |der_type|nameplate_rating_mw|status   |project_id|utility_id|\n+-----------+--------+-------------------+---------+----------+----------+\n|36_14_25453|Solar   |60.0               |installed|309761    |utility2  |\n|36_14_25453|Solar   |60.0               |planned  |309761    |utility2  |\n|36_14_25453|Solar   |60.0               |installed|309761    |utility2  |\n|36_14_25453|Solar   |60.0               |planned  |309761    |utility2  |\n|36_14_25453|Solar   |60.0               |planned  |309761    |utility2  |\n|36_14_25453|Solar   |60.0               |installed|309761    |utility2  |\n|36_14_25453|Solar   |60.0               |planned  |309761    |utility2  |\n|36_14_25453|Solar   |60.0               |installed|309761    |utility2  |\n|36_14_25453|Solar   |30.0               |installed|91300     |utility2  |\n|36_14_25453|Solar   |30.0               |installed|91300     |utility2  |\n|36_14_25453|Solar   |30.0               |planned  |91300     |utility2  |\n|36_14_25453|Solar   |30.0               |planned  |91300     |utility2  |\n|36_14_25453|Solar   |30.0               |installed|91300     |utility2  |\n|36_14_25453|Solar   |30.0               |planned  |91300     |utility2  |\n|36_14_25453|Solar   |30.0               |installed|91300     |utility2  |\n|36_14_25453|Solar   |30.0               |planned  |91300     |utility2  |\n|36_14_25453|Solar   |20.0               |planned  |94174     |utility2  |\n|36_14_25453|Solar   |20.0               |installed|94174     |utility2  |\n|36_14_25453|Solar   |20.0               |installed|94174     |utility2  |\n|36_14_25453|Solar   |20.0               |installed|94174     |utility2  |\n+-----------+--------+-------------------+---------+----------+----------+\nonly showing top 20 rows\n\n=== Data Quality Summary ===\n+--------------------------+---------+---------------+----------------------+\n|last_refresh              |row_count|table          |note                  |\n+--------------------------+---------+---------------+----------------------+\n|2022-10-15 00:00:00       |7903     |feeders        |NULL                  |\n|2026-01-20 03:18:08.874573|287636   |der_records    |NULL                  |\n|NULL                      |7903     |feeder_capacity|Aggregated from silver|\n|NULL                      |287636   |der_details    |Detailed DER view     |\n+--------------------------+---------+---------------+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Test 1: Feeders with max_hosting_capacity_mw > 5 (platinum.feeder_capacity)\")\n",
    "api1_query = \"\"\"\n",
    "SELECT *\n",
    "FROM iedr_dev_platinum.feeder_capacity\n",
    "WHERE max_hosting_capacity_mw > 5\n",
    "ORDER BY max_hosting_capacity_mw DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "spark.sql(api1_query).show(truncate=False)\n",
    "\n",
    "print(\"\\nTest 2: All installed + planned DER for a specific feeder (platinum.der_details) \")\n",
    "\n",
    "feeder_example = \"36_14_25453\"  \n",
    "\n",
    "api2_query = f\"\"\"\n",
    "SELECT \n",
    "    feeder_id,\n",
    "    der_type,\n",
    "    nameplate_rating_mw,\n",
    "    status,\n",
    "    project_id,\n",
    "    utility_id\n",
    "FROM iedr_dev_platinum.der_details\n",
    "WHERE feeder_id = '{feeder_example}'\n",
    "ORDER BY nameplate_rating_mw DESC\n",
    "\"\"\"\n",
    "spark.sql(api2_query).show(truncate=False)\n",
    "\n",
    "print(\"\\n=== Data Quality Summary ===\")\n",
    "spark.table(\"iedr_dev_platinum.data_quality_summary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be41365-3825-4d58-b5dc-ab9f125717e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Feeder aggregation logic → PASSED ✅\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import max as spark_max, count\n",
    "\n",
    "def test_feeder_aggregation():\n",
    "  \n",
    "    mock_data = [\n",
    "        (\"1001\", 4.8, 10.5, \"2022-10-01\"),   \n",
    "        (\"1001\", 4.8, 12.0, \"2022-10-01\"),   \n",
    "        (\"1002\", 4.8, 8.0, \"2022-10-02\"),    \n",
    "        (\"1002\", 4.8, 9.5, \"2022-10-02\"),    \n",
    "        (\"1001\", 4.8, 7.0, \"2022-10-01\")     \n",
    "    ]\n",
    "    \n",
    "    columns = [\"NYHCPV_csv_NFEEDER\", \"NYHCPV_csv_NVOLTAGE\", \"NYHCPV_csv_NMAXHC\", \"NYHCPV_csv_FHCADATE\"]\n",
    "    df = spark.createDataFrame(mock_data, columns)\n",
    "    \n",
    "    # Your aggregation logic (copy-paste from your notebook)\n",
    "    aggregated = df.groupBy(\"NYHCPV_csv_NFEEDER\").agg(\n",
    "        spark_max(\"NYHCPV_csv_NMAXHC\").alias(\"max_hosting_capacity_mw\"),\n",
    "        count(\"*\").alias(\"segment_count\")\n",
    "    )\n",
    "    \n",
    "    # Collect results\n",
    "    result = aggregated.orderBy(\"NYHCPV_csv_NFEEDER\").collect()\n",
    "    \n",
    "    # Assertions\n",
    "    assert len(result) == 2, \"Should have 2 unique feeders\"\n",
    "    \n",
    "    feeder1 = result[0]\n",
    "    assert feeder1[\"NYHCPV_csv_NFEEDER\"] == \"1001\"\n",
    "    assert feeder1[\"max_hosting_capacity_mw\"] == 12.0, \"Max HC for feeder1 wrong\"\n",
    "    assert feeder1[\"segment_count\"] == 3, \"Segment count for feeder1 wrong\"\n",
    "    \n",
    "    feeder2 = result[1]\n",
    "    assert feeder2[\"NYHCPV_csv_NFEEDER\"] == \"1002\"\n",
    "    assert feeder2[\"max_hosting_capacity_mw\"] == 9.5\n",
    "    assert feeder2[\"segment_count\"] == 2\n",
    "    \n",
    "    print(\"Test 2: Feeder aggregation logic → PASSED ✅\")\n",
    "\n",
    "# Run the test\n",
    "test_feeder_aggregation()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6606526481280628,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_transformation",
   "widgets": {
    "env": {
     "currentValue": "prod",
     "nuid": "6e3c5433-0019-4c37-87a7-58d219547b7f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "qa",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "qa",
        "prod"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}